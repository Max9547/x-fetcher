#!/usr/bin/env python3
"""
X (Twitter) å¸–å­å†…å®¹æŠ“å–å·¥å…·
æ”¯æŒæ™®é€šæ¨æ–‡å’Œ X Article é•¿æ–‡ç« ï¼Œå¯é€‰æŠ“å–è¯„è®º
ç”¨æ³•: python fetch_x.py <x_url> [é€‰é¡¹]
"""

import sys
import re
import json
import os
from datetime import datetime
import requests
from urllib.parse import urlparse

def extract_tweet_id(url):
    """ä» URL æå– tweet ID"""
    patterns = [
        r'(?:x\.com|twitter\.com)/\w+/status/(\d+)',
        r'(?:x\.com|twitter\.com)/\w+/statuses/(\d+)',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

def extract_username(url):
    """ä» URL æå–ç”¨æˆ·å"""
    match = re.search(r'(?:x\.com|twitter\.com)/(\w+)/status', url)
    return match.group(1) if match else None

def fetch_via_fxtwitter(url):
    """é€šè¿‡ fxtwitter API è·å–å†…å®¹"""
    api_url = re.sub(r'(x\.com|twitter\.com)', 'api.fxtwitter.com', url)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    try:
        resp = requests.get(api_url, headers=headers, timeout=15)
        if resp.status_code == 200:
            return resp.json()
    except Exception as e:
        print(f"  fxtwitter é”™è¯¯: {e}", file=sys.stderr)
    return None

def fetch_via_syndication(tweet_id):
    """é€šè¿‡ X çš„ syndication API è·å–å†…å®¹"""
    url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            return resp.json()
    except Exception as e:
        print(f"  syndication é”™è¯¯: {e}", file=sys.stderr)
    return None

def fetch_replies_via_syndication(tweet_id):
    """é€šè¿‡ syndication API è·å–è¯„è®º/å›å¤"""
    url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            # syndication API è¿”å›çš„æ•°æ®ä¸­å¯èƒ½åŒ…å«éƒ¨åˆ†å›å¤
            replies = []
            if "conversation" in data:
                for item in data.get("conversation", []):
                    if item.get("id_str") != tweet_id:
                        replies.append({
                            "id": item.get("id_str", ""),
                            "text": item.get("text", ""),
                            "author": item.get("user", {}).get("name", ""),
                            "username": item.get("user", {}).get("screen_name", ""),
                            "created_at": item.get("created_at", ""),
                            "likes": item.get("favorite_count", 0),
                            "retweets": item.get("retweet_count", 0)
                        })
            return replies
    except Exception as e:
        print(f"  è·å–è¯„è®ºé”™è¯¯: {e}", file=sys.stderr)
    return []

def fetch_replies_via_fxtwitter(tweet_id, username):
    """é€šè¿‡å¤šç§æ–¹å¼å°è¯•è·å–è¯„è®º"""
    replies = []
    
    # æ–¹æ³•1: å°è¯• syndication conversation
    syndication_replies = fetch_replies_via_syndication(tweet_id)
    if syndication_replies:
        replies.extend(syndication_replies)
    
    # æ–¹æ³•2: å°è¯•é€šè¿‡æœç´¢ API è·å–å›å¤ (ä½¿ç”¨ nitter å®ä¾‹)
    nitter_instances = [
        "nitter.poast.org",
        "nitter.privacydev.net",
    ]
    
    for instance in nitter_instances:
        try:
            search_url = f"https://{instance}/{username}/status/{tweet_id}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            resp = requests.get(search_url, headers=headers, timeout=10)
            if resp.status_code == 200:
                # ç®€å•è§£æå›å¤ï¼ˆnitter é¡µé¢ç»“æ„ï¼‰
                html = resp.text
                # æŸ¥æ‰¾å›å¤åŒºåŸŸ
                reply_pattern = r'class="reply-thread".*?class="tweet-content[^"]*"[^>]*>([^<]+)<'
                reply_matches = re.findall(reply_pattern, html, re.DOTALL)
                for i, text in enumerate(reply_matches[:20]):  # æœ€å¤š20æ¡
                    text = text.strip()
                    if text and len(text) > 5:
                        replies.append({
                            "id": f"reply_{i}",
                            "text": text,
                            "author": "Unknown",
                            "username": "unknown",
                            "created_at": "",
                            "likes": 0,
                            "retweets": 0
                        })
                if replies:
                    break
        except Exception as e:
            continue
    
    return replies

def extract_article_content(article):
    """ä» X Article ä¸­æå–å®Œæ•´å†…å®¹"""
    if not article:
        return None

    content_blocks = article.get("content", {}).get("blocks", [])

    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬å—
    paragraphs = []
    for block in content_blocks:
        text = block.get("text", "").strip()
        block_type = block.get("type", "unstyled")

        if text:
            # æ ¹æ®ç±»å‹æ·»åŠ æ ¼å¼
            if block_type == "header-one":
                paragraphs.append(f"# {text}")
            elif block_type == "header-two":
                paragraphs.append(f"## {text}")
            elif block_type == "header-three":
                paragraphs.append(f"### {text}")
            elif block_type == "blockquote":
                paragraphs.append(f"> {text}")
            elif block_type == "unordered-list-item":
                paragraphs.append(f"- {text}")
            elif block_type == "ordered-list-item":
                paragraphs.append(f"1. {text}")
            else:
                paragraphs.append(text)

    return "\n\n".join(paragraphs)

def format_output(data, source):
    """æ ¼å¼åŒ–è¾“å‡º"""
    result = {
        "source": source,
        "success": True,
        "type": "tweet",
        "content": {}
    }

    if source == "fxtwitter":
        tweet = data.get("tweet", {})
        article = tweet.get("article")

        if article:
            # X Article é•¿æ–‡ç« 
            result["type"] = "article"
            result["content"] = {
                "title": article.get("title", ""),
                "preview": article.get("preview_text", ""),
                "full_text": extract_article_content(article),
                "cover_image": article.get("cover_media", {}).get("media_info", {}).get("original_img_url"),
                "author": tweet.get("author", {}).get("name", ""),
                "username": tweet.get("author", {}).get("screen_name", ""),
                "created_at": article.get("created_at", ""),
                "modified_at": article.get("modified_at", ""),
                "likes": tweet.get("likes", 0),
                "retweets": tweet.get("retweets", 0),
                "views": tweet.get("views", 0),
                "bookmarks": tweet.get("bookmarks", 0)
            }
        else:
            # æ™®é€šæ¨æ–‡
            result["content"] = {
                "text": tweet.get("text", ""),
                "author": tweet.get("author", {}).get("name", ""),
                "username": tweet.get("author", {}).get("screen_name", ""),
                "created_at": tweet.get("created_at", ""),
                "likes": tweet.get("likes", 0),
                "retweets": tweet.get("retweets", 0),
                "views": tweet.get("views", 0),
                "media": [m.get("url") for m in tweet.get("media", {}).get("all", []) if m.get("url")],
                "replies": tweet.get("replies", 0)
            }

    elif source == "syndication":
        result["content"] = {
            "text": data.get("text", ""),
            "author": data.get("user", {}).get("name", ""),
            "username": data.get("user", {}).get("screen_name", ""),
            "created_at": data.get("created_at", ""),
            "likes": data.get("favorite_count", 0),
            "retweets": data.get("retweet_count", 0),
            "media": [m.get("media_url_https") for m in data.get("mediaDetails", []) if m.get("media_url_https")]
        }

    return result

def fetch_tweet(url):
    """ä¸»å‡½æ•°ï¼šå°è¯•å¤šç§æ–¹å¼è·å–å¸–å­å†…å®¹"""
    tweet_id = extract_tweet_id(url)
    username = extract_username(url)

    if not tweet_id:
        return {"success": False, "error": "æ— æ³•ä» URL æå– tweet ID"}, tweet_id, username

    print(f"ğŸ“ Tweet ID: {tweet_id}", file=sys.stderr)
    print(f"ğŸ“ Username: {username}", file=sys.stderr)
    print(f"ğŸ” æ­£åœ¨æŠ“å–...", file=sys.stderr)

    # æ–¹æ³•1: fxtwitter API (æ”¯æŒ Article)
    print("  å°è¯• fxtwitter API...", file=sys.stderr)
    data = fetch_via_fxtwitter(url)
    if data and data.get("tweet"):
        print("  âœ… fxtwitter æˆåŠŸ", file=sys.stderr)
        return format_output(data, "fxtwitter"), tweet_id, username

    # æ–¹æ³•2: syndication API
    print("  å°è¯• syndication API...", file=sys.stderr)
    data = fetch_via_syndication(tweet_id)
    if data and data.get("text"):
        print("  âœ… syndication æˆåŠŸ", file=sys.stderr)
        return format_output(data, "syndication"), tweet_id, username

    return {"success": False, "error": "æ‰€æœ‰æŠ“å–æ–¹å¼å‡å¤±è´¥"}, tweet_id, username


def generate_markdown(result, tweet_id, username, url, replies=None, include_replies=False):
    """ç”Ÿæˆ Markdown æ ¼å¼å†…å®¹"""
    content = result.get("content", {})
    content_type = result.get("type", "tweet")
    
    lines = []
    
    if content_type == "article":
        # X Article é•¿æ–‡ç« 
        title = content.get("title", "Untitled")
        lines.append(f"# {title}")
        lines.append("")
        lines.append(f"> ä½œè€…: **{content.get('author', '')}** (@{content.get('username', '')})")
        lines.append(f"> å‘å¸ƒæ—¶é—´: {content.get('created_at', '')}")
        if content.get('modified_at'):
            lines.append(f"> ä¿®æ”¹æ—¶é—´: {content.get('modified_at', '')}")
        lines.append(f"> åŸæ–‡é“¾æ¥: {url}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # å°é¢å›¾
        if content.get("cover_image"):
            lines.append(f"![å°é¢]({content.get('cover_image')})")
            lines.append("")
        
        # æ­£æ–‡
        if content.get("full_text"):
            lines.append(content.get("full_text"))
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("## äº’åŠ¨æ•°æ®")
        lines.append("")
        lines.append(f"- â¤ï¸ ç‚¹èµ: {content.get('likes', 0):,}")
        lines.append(f"- ğŸ” è½¬å‘: {content.get('retweets', 0):,}")
        lines.append(f"- ğŸ‘€ æµè§ˆ: {content.get('views', 0):,}")
        lines.append(f"- ğŸ”– ä¹¦ç­¾: {content.get('bookmarks', 0):,}")
    else:
        # æ™®é€šæ¨æ–‡
        lines.append(f"# @{content.get('username', '')} çš„æ¨æ–‡")
        lines.append("")
        lines.append(f"> ä½œè€…: **{content.get('author', '')}** (@{content.get('username', '')})")
        lines.append(f"> å‘å¸ƒæ—¶é—´: {content.get('created_at', '')}")
        lines.append(f"> åŸæ–‡é“¾æ¥: {url}")
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append(content.get("text", ""))
        lines.append("")
        
        # åª’ä½“
        media = content.get("media", [])
        if media:
            lines.append("## åª’ä½“")
            lines.append("")
            for i, m in enumerate(media, 1):
                lines.append(f"![åª’ä½“{i}]({m})")
            lines.append("")
        
        lines.append("---")
        lines.append("")
        lines.append("## äº’åŠ¨æ•°æ®")
        lines.append("")
        lines.append(f"- â¤ï¸ ç‚¹èµ: {content.get('likes', 0):,}")
        lines.append(f"- ğŸ” è½¬å‘: {content.get('retweets', 0):,}")
        lines.append(f"- ğŸ‘€ æµè§ˆ: {content.get('views', 0):,}")
        lines.append(f"- ğŸ’¬ å›å¤: {content.get('replies', 0):,}")
    
    # æ·»åŠ è¯„è®ºéƒ¨åˆ†
    if include_replies and replies:
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("## è¯„è®º/å›å¤")
        lines.append("")
        for i, reply in enumerate(replies, 1):
            lines.append(f"### {i}. @{reply.get('username', 'unknown')}")
            if reply.get('author'):
                lines.append(f"**{reply.get('author')}**")
            lines.append("")
            lines.append(reply.get('text', ''))
            lines.append("")
            if reply.get('likes') or reply.get('retweets'):
                lines.append(f"*â¤ï¸ {reply.get('likes', 0)} | ğŸ” {reply.get('retweets', 0)}*")
            lines.append("")
    
    return "\n".join(lines)


def save_markdown(markdown_content, tweet_id, username, suffix=""):
    """ä¿å­˜ Markdown æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if suffix:
        filename = f"{username}_{tweet_id}_{suffix}_{timestamp}.md"
    else:
        filename = f"{username}_{tweet_id}_{timestamp}.md"
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(markdown_content)
    
    return filename


def interactive_menu(result, tweet_id, username, url):
    """äº¤äº’å¼èœå•ï¼Œè®©ç”¨æˆ·é€‰æ‹©è¦æŠ“å–çš„å†…å®¹"""
    content = result.get("content", {})
    content_type = result.get("type", "tweet")
    
    print("\n" + "="*50, file=sys.stderr)
    print("ğŸ“‹ æŠ“å–æˆåŠŸï¼è¯·é€‰æ‹©è¦ä¿å­˜çš„å†…å®¹ï¼š", file=sys.stderr)
    print("="*50, file=sys.stderr)
    print("", file=sys.stderr)
    print("  [1] ä»…ä¿å­˜ä¸»è´´å†…å®¹", file=sys.stderr)
    print("  [2] ä»…ä¿å­˜è¯„è®º/å›å¤", file=sys.stderr)
    print("  [3] ä¿å­˜ä¸»è´´ + è¯„è®ºï¼ˆå®Œæ•´å½’æ¡£ï¼‰", file=sys.stderr)
    print("  [4] ä»…è¾“å‡º JSONï¼ˆä¸ä¿å­˜æ–‡ä»¶ï¼‰", file=sys.stderr)
    print("  [0] é€€å‡º", file=sys.stderr)
    print("", file=sys.stderr)
    
    choice = input("è¯·è¾“å…¥é€‰é¡¹ (0-4): ").strip()
    
    replies = []
    
    if choice in ['2', '3']:
        print("\nğŸ” æ­£åœ¨æŠ“å–è¯„è®º...", file=sys.stderr)
        replies = fetch_replies_via_fxtwitter(tweet_id, username)
        if replies:
            print(f"  âœ… è·å–åˆ° {len(replies)} æ¡è¯„è®º", file=sys.stderr)
        else:
            print("  âš ï¸ æœªèƒ½è·å–åˆ°è¯„è®ºï¼ˆå¯èƒ½æ˜¯ API é™åˆ¶æˆ–æ— è¯„è®ºï¼‰", file=sys.stderr)
    
    if choice == '1':
        # ä»…ä¸»è´´
        markdown_content = generate_markdown(result, tweet_id, username, url, replies=None, include_replies=False)
        filename = save_markdown(markdown_content, tweet_id, username, "post")
        print(f"\nâœ… ä¸»è´´å·²ä¿å­˜åˆ°: {filename}", file=sys.stderr)
        
    elif choice == '2':
        # ä»…è¯„è®º
        if replies:
            lines = [f"# @{username} æ¨æ–‡çš„è¯„è®º", ""]
            lines.append(f"> åŸæ–‡é“¾æ¥: {url}")
            lines.append(f"> æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            lines.append("")
            lines.append("---")
            lines.append("")
            for i, reply in enumerate(replies, 1):
                lines.append(f"## {i}. @{reply.get('username', 'unknown')}")
                if reply.get('author'):
                    lines.append(f"**{reply.get('author')}**")
                lines.append("")
                lines.append(reply.get('text', ''))
                lines.append("")
                if reply.get('likes') or reply.get('retweets'):
                    lines.append(f"*â¤ï¸ {reply.get('likes', 0)} | ğŸ” {reply.get('retweets', 0)}*")
                lines.append("")
            markdown_content = "\n".join(lines)
            filename = save_markdown(markdown_content, tweet_id, username, "replies")
            print(f"\nâœ… è¯„è®ºå·²ä¿å­˜åˆ°: {filename}", file=sys.stderr)
        else:
            print("\nâš ï¸ æ²¡æœ‰è¯„è®ºå¯ä¿å­˜", file=sys.stderr)
            
    elif choice == '3':
        # å®Œæ•´å½’æ¡£
        markdown_content = generate_markdown(result, tweet_id, username, url, replies=replies, include_replies=True)
        filename = save_markdown(markdown_content, tweet_id, username, "full")
        print(f"\nâœ… å®Œæ•´å½’æ¡£å·²ä¿å­˜åˆ°: {filename}", file=sys.stderr)
        
    elif choice == '4':
        # ä»… JSON
        output = {
            "tweet": result,
            "replies": replies if replies else []
        }
        print(json.dumps(output, ensure_ascii=False, indent=2))
        
    elif choice == '0':
        print("\nğŸ‘‹ å·²é€€å‡º", file=sys.stderr)
    else:
        print("\nâš ï¸ æ— æ•ˆé€‰é¡¹", file=sys.stderr)


def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python fetch_x.py <x_url> [é€‰é¡¹]")
        print("")
        print("é€‰é¡¹:")
        print("  --save-md        ç›´æ¥ä¿å­˜ä¸»è´´ä¸º Markdown")
        print("  --with-replies   åŒæ—¶æŠ“å–è¯„è®º")
        print("  --full           ä¿å­˜å®Œæ•´å½’æ¡£ï¼ˆä¸»è´´+è¯„è®ºï¼‰")
        print("  --json           ä»…è¾“å‡º JSONï¼Œä¸ä¿å­˜æ–‡ä»¶")
        print("")
        print("ç¤ºä¾‹:")
        print("  python fetch_x.py https://x.com/elonmusk/status/123456789")
        print("  python fetch_x.py https://x.com/elonmusk/status/123456789 --full")
        sys.exit(1)

    url = sys.argv[1]
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    save_md = "--save-md" in sys.argv
    with_replies = "--with-replies" in sys.argv
    full_archive = "--full" in sys.argv
    json_only = "--json" in sys.argv
    
    result, tweet_id, username = fetch_tweet(url)

    if not result.get("success"):
        print(json.dumps(result, ensure_ascii=False, indent=2))
        sys.exit(1)
    
    # æ ¹æ®å‚æ•°å†³å®šè¡Œä¸º
    if json_only:
        # ä»…è¾“å‡º JSON
        replies = []
        if with_replies or full_archive:
            print("ğŸ” æ­£åœ¨æŠ“å–è¯„è®º...", file=sys.stderr)
            replies = fetch_replies_via_fxtwitter(tweet_id, username)
        output = {"tweet": result, "replies": replies}
        print(json.dumps(output, ensure_ascii=False, indent=2))
        
    elif full_archive:
        # å®Œæ•´å½’æ¡£æ¨¡å¼
        print("ğŸ” æ­£åœ¨æŠ“å–è¯„è®º...", file=sys.stderr)
        replies = fetch_replies_via_fxtwitter(tweet_id, username)
        if replies:
            print(f"  âœ… è·å–åˆ° {len(replies)} æ¡è¯„è®º", file=sys.stderr)
        markdown_content = generate_markdown(result, tweet_id, username, url, replies=replies, include_replies=True)
        filename = save_markdown(markdown_content, tweet_id, username, "full")
        print(f"âœ… å®Œæ•´å½’æ¡£å·²ä¿å­˜åˆ°: {filename}", file=sys.stderr)
        
    elif save_md:
        # ä»…ä¿å­˜ä¸»è´´
        replies = []
        if with_replies:
            print("ğŸ” æ­£åœ¨æŠ“å–è¯„è®º...", file=sys.stderr)
            replies = fetch_replies_via_fxtwitter(tweet_id, username)
        markdown_content = generate_markdown(result, tweet_id, username, url, replies=replies, include_replies=with_replies)
        filename = save_markdown(markdown_content, tweet_id, username)
        print(f"âœ… å·²ä¿å­˜åˆ°: {filename}", file=sys.stderr)
        
    else:
        # äº¤äº’æ¨¡å¼
        print(json.dumps(result, ensure_ascii=False, indent=2))
        interactive_menu(result, tweet_id, username, url)


if __name__ == "__main__":
    main()
